# Agentic AI Personal Assistant

A sophisticated AI personal assistant built with Python, LangChain, and LangGraph, featuring long-term memory and a modern Streamlit interface.

## ğŸ—ï¸ Project Architecture

```
personal_assistant/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ agent.py          # Main agent implementation
â”‚   â”‚   â”œâ”€â”€ memory.py         # Long-term memory system
â”‚   â”‚   â””â”€â”€ config.py         # Configuration management
â”‚   â”œâ”€â”€ chains/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ chat_chain.py     # LangChain chat implementation
â”‚   â”œâ”€â”€ graphs/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ workflow.py       # LangGraph workflow definitions
â”‚   â””â”€â”€ ui/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ streamlit_app.py  # Streamlit interface
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_agent.py
â”‚   â”œâ”€â”€ test_memory.py
â”‚   â””â”€â”€ test_chat_chain.py
â”œâ”€â”€ data/
â”‚   â””â”€â”€ memory/               # Memory storage directory
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env.example
â”œâ”€â”€ .gitignore
â”œâ”€â”€ main.py                   # Application entry point
â””â”€â”€ README.md
```

## ğŸ§  LangGraph Architecture

Our assistant uses LangGraph to create a sophisticated workflow that orchestrates multiple AI agents and tools:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   User Input    â”‚â”€â”€â”€â–¶â”‚  Input Router   â”‚â”€â”€â”€â–¶â”‚  Memory Agent   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚                        â”‚
                                â–¼                        â–¼
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚  Chat Agent     â”‚    â”‚  Memory Store   â”‚
                       â”‚  (Groq LLM)     â”‚    â”‚  (Vector DB)    â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚                        â”‚
                                â–¼                        â–¼
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚  Response      â”‚    â”‚  Memory Update  â”‚
                       â”‚  Generator     â”‚    â”‚  & Storage      â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚  User Output   â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Features

- **Intelligent Chat Interface**: Powered by Groq's fast LLM inference
- **Long-term Memory**: Persistent memory using vector databases
- **Agentic Workflows**: LangGraph-powered multi-agent orchestration
- **Modern UI**: Clean Streamlit interface with real-time chat
- **Modular Architecture**: Easy to extend and customize

## ğŸ› ï¸ Technology Stack

- **Python 3.9+**
- **LangChain**: LLM orchestration and chains
- **LangGraph**: Multi-agent workflow management
- **Groq**: Fast LLM inference
- **Streamlit**: Web interface
- **ChromaDB**: Vector database for memory
- **Pydantic**: Data validation and settings

## ğŸ“¦ Installation

1. Clone the repository:
```bash
git clone <repository-url>
cd personal_assistant
```

2. Create a virtual environment:
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

3. Install dependencies:
```bash
pip install -r requirements.txt
```

4. Set up environment variables:
```bash
cp .env.example .env
# Edit .env with your Groq API key
```

## ğŸ”‘ Configuration

Create a `.env` file with the following variables:
```env
GROQ_API_KEY=your_groq_api_key_here
MEMORY_PERSIST_DIR=./data/memory
```

## ğŸš€ Usage

### Running the Streamlit App
```bash
streamlit run src/ui/streamlit_app.py
```

### Running from Command Line
```bash
python main.py
```

## ğŸ§ª Testing

Run the test suite:
```bash
pytest tests/
```

## ğŸ“ Project Structure Details

- **`src/core/`**: Core agent logic and memory management
- **`src/chains/`**: LangChain implementations for chat and reasoning
- **`src/graphs/`**: LangGraph workflow definitions and state management
- **`src/ui/`**: Streamlit interface components
- **`tests/`**: Comprehensive test suite
- **`data/`**: Persistent storage for memory and user data

## ğŸ”„ Workflow

1. **Input Processing**: User input is received and routed to appropriate agents
2. **Memory Retrieval**: Relevant context is retrieved from long-term memory
3. **LLM Processing**: Groq LLM processes the input with context
4. **Response Generation**: Intelligent response is generated and formatted
5. **Memory Update**: New information is stored in long-term memory
6. **Output Delivery**: Response is delivered to the user interface

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests
5. Submit a pull request

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ™ Acknowledgments

- LangChain team for the amazing framework
- LangGraph for workflow orchestration
- Groq for fast LLM inference
- Streamlit for the beautiful UI framework
